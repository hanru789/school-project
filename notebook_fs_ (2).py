# -*- coding: utf-8 -*-
"""notebook-FS-.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LzMJwpKOIU-MMufPBGPD5P-Mdps6uDk_

# Proyek Akhir: Menyelesaikan Permasalahan Perusahaan Edutech

- Nama: Ruhan
- Email:ruhanmasykuri@gmail.com
- Id Dicoding:hanru789

## Persiapan

### Menyiapkan library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

"""### Menyiapkan data yang akan diguankan

## Data Understanding
"""

data = pd.read_csv('data.csv', sep=';')
data.head()

print('Missing Value: \n', data.isna().sum())

print('Duplicated data: ',data.duplicated().sum())

print(data.columns)

cat = ['Marital_status', 'Application_mode', 'Course','Daytime_evening_attendance', 'Previous_qualification','Nacionality',
       'Mothers_qualification','Fathers_qualification', 'Mothers_occupation', 'Fathers_occupation','Displaced',
       'Educational_special_needs', 'Debtor','Tuition_fees_up_to_date', 'Gender', 'Scholarship_holder','International']


num = ['Application_order', 'Previous_qualification_grade', 'Admission_grade', 'Age_at_enrollment', 'Curricular_units_1st_sem_credited',
       'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved',
       'Curricular_units_1st_sem_grade', 'Curricular_units_1st_sem_without_evaluations', 'Curricular_units_2nd_sem_credited',
       'Curricular_units_2nd_sem_enrolled', 'Curricular_units_2nd_sem_evaluations', 'Curricular_units_2nd_sem_approved',
       'Curricular_units_2nd_sem_grade', 'Curricular_units_2nd_sem_without_evaluations']
lab =  ['Status']

for i in data[cat]:
  plt.figure(figsize=(10,5))
  sns.countplot(data=data, x=i)
  plt.show()

cat_corr = data[cat].corr()
plt.figure(figsize=(10,5))
sns.heatmap(cat_corr, annot=True)
plt.show()

for i in data[num]:
  plt.figure(figsize=(10,5))
  sns.histplot(data=data, x=i)
  plt.show()

num_cor = data[num].corr()
plt.figure(figsize=(10,5))
sns.heatmap(num_cor, annot=True)
plt.show()

plt.figure(figsize=(10,5))
sns.countplot(data=data, x='Status')
plt.show()

"""Data Understanding Insight :
- Tidak ada missing values
- Tidak ada duplicated data
- Numerical feature dilakukan normalisasi
- Categorical feature dilakukan one hot encoding
- Mayoritas marital_status adalah kategori 1 dan yang lainnya sangat sedikit, kolom ini dapat di drop
- Mayoritas Previous_quailfication adalah kategori 1 dan yang lain sangat sedikit, kolom ini dapat di drop
- Mayoritas Nacinality adalah kategori 1 dan yang lain sangat sedikit, kolom ini dapat di drop
- Kolom berikut pada numerical kolom dapat dilakukan PCA karena memiliki korelasi ['Curricular_units_1st_sem_enrolled','Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_credited', 'Curricular_units_2nd_sem_enrolled', 'Curricular_units_2nd_sem_evaluations', 'Curricular_units_2nd_sem_approved', 'Curricular_units_2nd_sem_grade',]

- Pada kolom status terdapat tiga kategori. Karena project ini difokuskan pada Dropout rate, maka kategori Graduate dan Enrolled dapat digabung saja. Pada akhirnya kolom status hanya berisi kategori 1 untuk Dropout dan 0 untuk not Dropout.
- Label status tidak seimbang, dilakukan under sampling pada not dropout

## Data Preparation / Preprocessing
"""

new_cat = ['Application_mode', 'Course','Daytime_evening_attendance',
       'Mothers_qualification','Fathers_qualification', 'Mothers_occupation', 'Fathers_occupation','Displaced',
       'Educational_special_needs', 'Debtor','Tuition_fees_up_to_date', 'Gender', 'Scholarship_holder','International']

new_data = pd.concat([data[new_cat], data[num], data[lab]], axis=1)

new_data.Status = new_data.Status.map({'Dropout': 1, 'Enrolled': 0, 'Graduate':0})

from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(new_data, test_size=0.2, random_state=42, shuffle=True)
print(train_data.shape)
print(test_data.shape)

print(train_data[lab].value_counts())
sns.countplot(data=train_data, x='Status')
plt.show()

from sklearn.utils import resample, shuffle
minority = train_data[train_data.Status == 1]
majority = train_data[train_data.Status == 0]
undersampled_majority = resample(majority, n_samples=minority.shape[0], random_state=42)
train_data = pd.concat([minority, undersampled_majority])
train_data = shuffle(train_data)

sns.countplot(data=train_data, x='Status')
plt.show()

train_label = train_data.Status
test_label = test_data.Status

test_data.drop(columns='Status', axis=1, inplace=True)
train_data.drop(columns='Status', axis=1, inplace=True)

import os

# Membuat folder 'model' jika belum ada
os.makedirs("model", exist_ok=True)

from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
from sklearn.preprocessing import MinMaxScaler
import joblib

def scaling(features, df, df_test=None):
    if df_test is not None:
        df = df.copy()
        df_test = df_test.copy()
        for feature in features:
            scaler = MinMaxScaler()
            X = np.asanyarray(df[feature])
            X = X.reshape(-1,1)
            scaler.fit(X)
            df["{}".format(feature)] = scaler.transform(X)
            joblib.dump(scaler, "model/scaler_{}.joblib".format(feature))

            X_test = np.asanyarray(df_test[feature])
            X_test = X_test.reshape(-1,1)
            df_test["{}".format(feature)] = scaler.transform(X_test)
        return df, df_test
    else:
        df = df.copy()
        for feature in features:
            scaler = MinMaxScaler()
            X = np.asanyarray(df[feature])
            X = X.reshape(-1,1)
            scaler.fit(X)
            df["{}".format(feature)] = scaler.transform(X)
            joblib.dump(scaler, "model/scaler_{}.joblib".format(feature))
        return df

from sklearn.preprocessing import OneHotEncoder
import pandas as pd
import joblib

def one_hot_encoding(features, df, df_test=None):
    df = df.copy()
    if df_test is not None:
        df_test = df_test.copy()

    for feature in features:
        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

        # Fit encoder di data train
        encoder.fit(df[[feature]])

        # Transform train
        ohe_train = encoder.transform(df[[feature]])
        ohe_train_df = pd.DataFrame(ohe_train, columns=[f"{feature}_{cat}" for cat in encoder.categories_[0]])
        ohe_train_df.index = df.index  # menjaga index tetap
        df = df.drop(columns=[feature])
        df = pd.concat([df, ohe_train_df], axis=1)

        # Transform test
        if df_test is not None:
            ohe_test = encoder.transform(df_test[[feature]])
            ohe_test_df = pd.DataFrame(ohe_test, columns=[f"{feature}_{cat}" for cat in encoder.categories_[0]])
            ohe_test_df.index = df_test.index
            df_test = df_test.drop(columns=[feature])
            df_test = pd.concat([df_test, ohe_test_df], axis=1)

        # Simpan encoder
        joblib.dump(encoder, f"model/onehot_{feature}.joblib")

    return (df, df_test) if df_test is not None else df

new_train_data, new_test_data = scaling(num, train_data, test_data)
new_train_data, new_test_data = one_hot_encoding(new_cat, new_train_data, new_test_data)

new_train_data.shape

pca_numerical = [
    'Curricular_units_1st_sem_enrolled','Curricular_units_1st_sem_evaluations',
    'Curricular_units_1st_sem_approved', 'Curricular_units_1st_sem_grade',
    'Curricular_units_2nd_sem_credited', 'Curricular_units_2nd_sem_enrolled',
    'Curricular_units_2nd_sem_evaluations', 'Curricular_units_2nd_sem_approved',
    'Curricular_units_2nd_sem_grade',]

train_pca_df = new_train_data.copy().reset_index(drop=True)
test_pca_df = new_test_data.copy().reset_index(drop=True)

from sklearn.decomposition import PCA

pca = PCA(n_components=len(pca_numerical), random_state=123)
pca.fit(train_pca_df[pca_numerical])
princ_comp = pca.transform(train_pca_df[pca_numerical])

var_exp = pca.explained_variance_ratio_.round(3)
cum_var_exp = np.cumsum(var_exp)

plt.bar(range(len(pca_numerical)), var_exp, alpha=0.5, align='center', label='individual explained variance')
plt.step(range(len(pca_numerical)), cum_var_exp, where='mid', label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

pca_1 = PCA(n_components=2, random_state=123)
pca_1.fit(train_pca_df[pca_numerical])
joblib.dump(pca_1, "model/pca_{}.joblib".format(1))
princ_comp_1 = pca_1.transform(train_pca_df[pca_numerical])
train_pca_df[["pc1_1", "pc1_2"]] = pd.DataFrame(princ_comp_1, columns=["pc1_1", "pc1_2"])
train_pca_df.drop(columns=pca_numerical, axis=1, inplace=True)
train_pca_df.head()

test_princ = pca_1.transform(test_pca_df[pca_numerical])
test_pca_df[["pc1_1", "pc1_2"]] = pd.DataFrame(test_princ, columns=["pc1_1", "pc1_2"])
test_pca_df.drop(columns=pca_numerical, axis=1, inplace=True)

test_pca_df.head()

"""## Modeling"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

param_grid = {
    "penalty": ["l1","l2"],
    "C": [0.01, 0.1, 1]
}

log_model = LogisticRegression(random_state=123)

CV_lr = GridSearchCV(estimator=log_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_lr.fit(train_pca_df, train_label)

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

tree_model = DecisionTreeClassifier(random_state=123)

param_grid = {
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [5, 6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_tree = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_tree.fit(train_pca_df, train_label)

from sklearn.ensemble import RandomForestClassifier

rdf_model = RandomForestClassifier(random_state=123)

param_grid = {
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_rdf = GridSearchCV(estimator=rdf_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_rdf.fit(train_pca_df, train_label)

print("best parameters: ", CV_rdf.best_params_)

rdf_model = RandomForestClassifier(
    random_state=123,
    max_depth=8,
    n_estimators=500,
    max_features='sqrt',
    criterion='gini',
    n_jobs=-1
)
rdf_model.fit(train_pca_df, train_label)
joblib.dump(rdf_model, "model/rdf_model.joblib")

from sklearn.ensemble import GradientBoostingClassifier

gboost_model = GradientBoostingClassifier(random_state=123)

param_grid = {
    'max_depth': [5, 8],
    'n_estimators': [200, 300],
    'learning_rate': [0.01, 0.1],
    'max_features': ['auto', 'sqrt', 'log2']
}

CV_gboost = GridSearchCV(estimator=gboost_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_gboost.fit(train_pca_df, train_label)

joblib.dump(CV_gboost, "model/gboost_model.joblib")

#import shutil
#shutil.rmtree('model')



"""## Evaluation"""

from sklearn.metrics import classification_report, confusion_matrix
prediction = rdf_model.predict(test_pca_df)
print(classification_report(test_label, prediction))

pip freeze > requirements.txt

